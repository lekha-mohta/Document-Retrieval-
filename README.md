# Document Retrieval System

## Overview
This project implements a basic document retrieval system based on the vector space model. The system supports multiple term weighting schemes and preprocessing options, enabling ranked retrieval for queries against a document collection. Performance is evaluated using precision, recall, and F-measure metrics.

## Features
- **Term Weighting Schemes:** Binary, Term Frequency (TF), and TF-IDF.
- **Preprocessing Options:**
  - Stoplisting
  - Stemming
- **Performance Evaluation:** Comparison of retrieved documents with a gold standard to compute precision, recall, and F-measure.
- **Customizable Retrieval:** Process queries using different configurations for weighting and preprocessing.

## Files
### Text Files
1. **`documents.txt`**
   - Collection of documents recording publications in the CACM (Communications of the Association for Computing Machinery).
   - Contains the title, author(s), and abstract for each publication (though some records may not include an abstract).

2. **`queries.txt`**
   - Contains a set of queries for retrieval testing.
   - Queries are written in "old-style" format, often as full paragraphs describing information needs.

3. **`cacm_gold_std.txt`**
   - The gold standard file that maps queries to relevant documents for performance evaluation.

### Python Files
1. **`IR_engine.py`**
   - Acts as the main entry point for the document retrieval system.
   - Processes queries using a preprocessed index stored in `IR_data.pickle`.
   - Supports preprocessing options (stoplisting and stemming) and allows selection of term weighting schemes (`binary`, `tf`, `tfidf`).
   - Outputs the 10 best-ranking documents for each query to a results file.

2. **`my_retriever.py`**
   - Implements the `Retrieve` class, which contains the core retrieval logic.
   - Responsible for ranking documents based on query similarity using vector space model calculations.
   - Includes methods for computing document vectors, term weighting, and cosine similarity.

3. **`eval_ir.py`**
   - Evaluates the performance of the retrieval system.
   - Compares the system-generated results against the gold standard to calculate precision, recall, and F-measure.

4. **`example_results_file.txt`**
   - Example output of the retrieval system showing ranked document lists for each query.

### Data File
1. **`IR_data.pickle`**
   - A serialized Python object containing the preprocessed index and query data.
   - Includes configurations for stoplisting and stemming.

## Usage
### Running the Retrieval Engine
To generate a results file:
```bash
python IR_engine.py -w <weighting_scheme> [-s] [-p] -o <output_file>
```
- `-w <weighting_scheme>`: Choose from `binary`, `tf`, `tfidf`.
- `-s`: Enable stoplisting.
- `-p`: Enable stemming.
- `-o <output_file>`: Specify the name of the results file.

**Example:**
```bash
python IR_engine.py -w tfidf -s -p -o results.txt
```

### Evaluating the Results
To evaluate the retrieval performance:
```bash
python eval_ir.py <gold_standard_file> <results_file>
```
- `<gold_standard_file>`: Path to `cacm_gold_std.txt`.
- `<results_file>`: Path to the results file generated by `IR_engine.py`.

**Example:**
```bash
python eval_ir.py cacm_gold_std.txt results.txt
```

### Help
To view available options for `IR_engine.py`:
```bash
python IR_engine.py -h
```
To view available options for `eval_ir.py`:
```bash
python eval_ir.py -h
```

## Implementation Details
The `Retrieve` class in `my_retriever.py` implements:
- Computation of document and query vectors.
- Calculation of cosine similarity for ranking documents.
- Preprocessing and term weighting logic.

### Preprocessing
- **Stoplisting:** Removes common stop words from the text.
- **Stemming:** Reduces words to their root forms.

### Vector Space Model
- Documents and queries are represented as term vectors.
- Cosine similarity measures the relevance of a document to a query.

## Examples
#### Command to Retrieve Documents with TF-IDF Weighting and Stemming:
```bash
python IR_engine.py -w tfidf -p -o tfidf_results.txt
```

#### Command to Evaluate Results:
```bash
python eval_ir.py cacm_gold_std.txt tfidf_results.txt
```

## Requirements
- Python 3.x
- No external libraries beyond standard Python modules (e.g., `math`, `re`, `sys`).

